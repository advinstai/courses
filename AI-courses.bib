
@book{geron_hands-machine_2017,
	address = {Sebastopol, CA},
	title = {Hands-on machine learning with {Scikit}-{Learn} and {TensorFlow} : concepts, tools, and techniques to build intelligent systems},
	isbn = {978-1-4919-6229-9},
	publisher = {O'Reilly Media},
	author = {Géron, Aurélien},
	year = {2017},
	keywords = {2017 book machine-learning oreilly tensorflow textbook}
}

@book{kelleher_fundamentals_2015,
	address = {Cambridge, Massachusetts},
	title = {Fundamentals of machine learning for predictive data analytics: algorithms, worked examples, and case studies},
	isbn = {978-0-262-02944-5},
	shorttitle = {Fundamentals of machine learning for predictive data analytics},
	publisher = {The MIT Press},
	author = {Kelleher, John D. and Mac Namee, Brian and D'Arcy, Aoife},
	year = {2015},
	keywords = {Data mining, Machine learning, Prediction theory},
	annote = {Machine learning for predictive data analytics -- Data to insights to decisions -- Data exploration -- Information-based learning -- Similarity-based learning -- Probability-based learning -- Error-based learning -- Evaluation -- Case study : customer churn -- Case study : galaxy classification -- The art of machine learning for predictive data analytics}
}

@misc{noauthor_introduction_nodate,
	title = {Introduction to {Data} {Mining}},
	url = {https://www-users.cs.umn.edu/~kumar001/dmbook/index.php},
	urldate = {2019-06-04},
	file = {Introduction to Data Mining:files/2303/index.html:text/html}
}

@book{tan_introduction_2019,
	address = {NY NY},
	edition = {Second edition},
	title = {Introduction to data mining},
	isbn = {978-0-13-312890-1},
	publisher = {Pearson},
	author = {Tan, Pang-Ning and Steinbach, Michael and Karpatne, Anuj and Kumar, Vipin},
	year = {2019},
	keywords = {Data mining}
}

@book{james_introduction_2013,
	address = {New York},
	series = {Springer texts in statistics},
	title = {An introduction to statistical learning: with applications in {R}},
	isbn = {978-1-4614-7137-0},
	shorttitle = {An introduction to statistical learning},
	number = {103},
	publisher = {Springer},
	editor = {James, Gareth and Witten, Daniela and Hastie, Trevor and Tibshirani, Robert},
	year = {2013},
	note = {OCLC: ocn828488009},
	keywords = {Mathematical models, Mathematical statistics, Problems, exercises, etc, R (Computer program language), Statistics},
	annote = {Includes index}
}

@book{raschka_python_nodate,
	address = {Birmingham Mumbai},
	edition = {Second edition, fourth release,[fully revised and updated]},
	series = {Expert insight},
	title = {Python machine learning: machine learning and deep learning with {Python}, scikit-learn, and {TensorFlow}},
	isbn = {978-1-78712-593-3},
	shorttitle = {Python machine learning},
	language = {eng},
	publisher = {Packt Publishing},
	author = {Raschka, Sebastian and Mirjalili, Vahid},
	month = apr,
	annote = {"Second edition, fully revised and updated" - auf dem Umschlag}
}

@book{vanderplas_python_2016,
	address = {Sebastopol, CA},
	edition = {First edition},
	title = {Python data science handbook: essential tools for working with data},
	isbn = {978-1-4919-1205-8},
	shorttitle = {Python data science handbook},
	publisher = {O'Reilly Media, Inc},
	author = {Vanderplas, Jacob T.},
	year = {2016},
	note = {OCLC: ocn915498936},
	keywords = {Data mining, Data Mining, Datenanalyse, Datenmanagement, Python, Python (Computer program language)},
	annote = {Includes index},
	annote = {IPython: beyond normal Python -- Introduction to NumPy -- Data manipulation with Pandas -- Visualization with Matplotlib -- Machine learning}
}

@book{ferrari_introducao_2017,
	edition = {Edição: 1},
	title = {Introdução a mineração de dados},
	abstract = {A Mineração de Dados surgiu como área de pesquisa e aplicação independente em meados da década de 1990, mas suas origens na matemática, estatística e computação são muito anteriores a esse período. A área também ganhou evidência nos últimos anos após ser cunhado o termo Big Data e com a publicação do relatório intitulado ?Big Data: The Next Frontier for Innovation, Competition, and Productivity? pelo McKinsey Global Institute em meados de 2011. A mineração de dados é o elemento central responsável pela parte analítica (do inglês data analytics) do Big Data, ou seja, pela preparação e análise das grandes massas de dados. Com a nova nomenclatura, até os profissionais que atuam na área ganharam novo nome: analistas de dados (do inglês data analysts), ou cientistas de dados (do inglês data scientists). E esses profissionais são cada vez mais requisitados e bem pagos, principalmente no momento em que o volume de dados produzidos cresce exponencialmente, ao ponto de que em curtos períodos de tempo se gera mais dados do que em muitos séculos de história da humanidade. Para esse crescimento não parece haver limites e as oportunidades acadêmicas e comerciais da área surgem também em grande variedade, velocidade e volume!Apesar da importância atual da área, a maior parte da literatura disponível está escrita em inglês e ainda são raros seus cursos de formação. Em Português há poucos livros, alguns poucos textos em análise de dados que possuem uma abordagem bastante distinta da proposta deste livro. Ao mesmo tempo, começam a surgir disciplinas específicas de graduação e pós-graduação em mineração de dados, análise de dados, Big Data e outras relacionadas. É nesse contexto que surge o livro ?Introdução a Mineração de Dados: Conceitos Básicos, Algoritmos e Aplicações?, com a proposta de ser uma referência introdutória a área, voltada para estudantes e profissionais das ciências exatas, humanas e sociais aplicadas. Este livro foi escrito como texto básico para a área no idioma Português e está pautado numa escrita acessível a alunos de graduação, pós-graduação e profissionais que atuam ou querem atuar na área.},
	language = {Português},
	publisher = {SARAIVA},
	author = {FERRARI, LEANDRO NUNES DE CASTRO SILVA DANIEL GOMES},
	year = {2017}
}

@misc{leandro_de_castro_2016:_nodate,
	type = {Dados e análise},
	title = {2016: {Introdução} à {Mineração} de {Dados}: {Conceitos} {Básicos}, {Algoritmos} …},
	shorttitle = {2016},
	url = {https://pt.slideshare.net/lndecastro/2016-introduo-minerao-de-dados-conceitos-bsicos-algoritmos-e-aplicaes},
	abstract = {Conjunto de slides desenvolvido como material de apoio disponível para uso por},
	urldate = {2019-06-04},
	author = {Leandro de Castro}
}

@book{silva_introducao_2017,
	title = {Introdução à {Mineração} de {Dados}: {Com} {Aplicações} em {R}},
	shorttitle = {Introdução à {Mineração} de {Dados}},
	abstract = {A quantidade de dados gerada atualmente tem extrapolado a capacidade humana de interpretação. O armazenamento de todo tipo de informação que antes era objeto de desejo de grandes e até médias empresas, agora se torna um desafio de como analisar essa superabundância de dados. A este desafio em específico está o interesse em determinar ações estratégicas, visando à descoberta de conhecimento em bases de dados para aumentar vendas, definir perfis e sugerir produtos relacionados. A descoberta de conhecimento constitui-se de um processo, cuja primeira etapa tem o objetivo de fazer um pré-processamento na base de dados para entregar a fase seguinte os dados limpos, preparados e selecionados. A fase seguinte, que é principal, esta a Mineração de Dados. Nessa etapa, algoritmos de aprendizado de máquina ou de redes neurais artificiais são executados sobre os dados, a fim de criar um modelo que auxilie em tarefas como classificação, agrupamento e associação de dados. Finalmente, como última etapa, os resultados da mineração são interpretados e analisados qualitativamente e quantitativamente. Diante o exposto, nota-se que é uma área interdisciplinar e exige do leitor uma grande diversidade de experiências que envolvem, basicamente: banco de dados, álgebra linear, matemática discreta e algoritmos. Nesse sentido, esta obra tem como objetivo a apresentação destes assuntos de forma contextualizada, de modo a facilitar o entendimento de um problema e sua resolução através de algoritmos escritos em pseudo-códigos e executados em passo a passo. Adicionalmente, os problemas resolvidos analiticamente são também simulados em uma ferramenta case. Com estas estratégias, esta obra constitui-se de uma visão bastante pragmática dos algoritmos de Mineração de Dados e suas utilizações em estudos de casos reais resolvidos e simulados.},
	language = {Português},
	publisher = {Elsevier Academic},
	author = {Silva, Leandro Augusto da and Peres, Sarajane Marques and Boscarioli, Clodis},
	year = {2017}
}

@book{kelleher_fundamentals_2015-1,
	address = {Cambridge, Massachusetts},
	title = {Fundamentals of {Machine} {Learning} for {Predictive} {Data} {Analytics} – {Algorithms}, {Worked} {Examples}, and {Case} {Studies}},
	isbn = {978-0-262-02944-5},
	abstract = {A comprehensive introduction to the most important machine learning approaches used in predictive data analytics, covering both theoretical concepts and practical applications.Machine learning is often used to build predictive models by extracting patterns from large datasets. These models are used in predictive data analytics applications including price prediction, risk assessment, predicting customer behavior, and document classification. This introductory textbook offers a detailed and focused treatment of the most important machine learning approaches used in predictive data analytics, covering both theoretical concepts and practical applications. Technical and mathematical material is augmented with explanatory worked examples, and case studies illustrate the application of these models in the broader business context.After discussing the trajectory from data to insight to decision, the book describes four approaches to machine learning: information-based learning, similarity-based learning, probability-based learning, and error-based learning. Each of these approaches is introduced by a nontechnical explanation of the underlying concept, followed by mathematical models and algorithms illustrated by detailed worked examples. Finally, the book considers techniques for evaluating prediction models and offers two case studies that describe specific data analytics projects through each phase of development, from formulating the business problem to implementation of the analytics solution. The book, informed by the authors' many years of teaching machine learning, and working on predictive data analytics projects, is suitable for use by undergraduates in computer science, engineering, mathematics, or statistics; by graduate students in disciplines with applications for predictive data analytics; and as a reference for professionals.},
	language = {Inglês},
	publisher = {MIT Press},
	author = {Kelleher, John D. and Namee, Brian Mac and D`arcy, Aoife},
	year = {2015}
}

@book{hope_learning_2017,
	address = {Sebastopol, CA},
	edition = {Edição: 1},
	title = {Learning {TensorFlow}: {A} {Guide} to {Building} {Deep} {Learning} {Systems}},
	isbn = {978-1-4919-7851-1},
	shorttitle = {Learning {TensorFlow}},
	abstract = {Roughly inspired by the human brain, deep neural networks trained with large amounts of data can solve complex tasks with unprecedented accuracy. This practical book provides an end-to-end guide to TensorFlow, the leading open source software library that helps you build and train neural networks for computer vision, natural language processing (NLP), speech recognition, and general predictive analytics.Authors Tom Hope, Yehezkel Resheff, and Itay Lieder provide a hands-on approach to TensorFlow fundamentals for a broad technical audience—from data scientists and engineers to students and researchers. You’ll begin by working through some basic examples in TensorFlow before diving deeper into topics such as neural network architectures, TensorBoard visualization, TensorFlow abstraction libraries, and multithreaded input pipelines. Once you finish this book, you’ll know how to build and deploy production-ready deep learning systems in TensorFlow.Get up and running with TensorFlow, rapidly and painlesslyLearn how to use TensorFlow to build deep learning models from the ground upTrain popular deep learning models for computer vision and NLPUse extensive abstraction libraries to make development easier and fasterLearn how to scale TensorFlow, and use clusters to distribute model trainingDeploy TensorFlow in a production setting},
	language = {English},
	publisher = {O'Reilly Media},
	author = {Hope, Tom and Resheff, Yehezkel S. and Lieder, Itay},
	year = {2017}
}

@book{teetor_r_2011,
	address = {Beijing ; Sebastopol, CA},
	edition = {Edição: 1},
	title = {R {Cookbook}: {Proven} {Recipes} for {Data} {Analysis}, {Statistics}, and {Graphics}},
	isbn = {978-0-596-80915-7},
	shorttitle = {R {Cookbook}},
	abstract = {With more than 200 practical recipes, this book helps you perform data analysis with R quickly and efficiently. The R language provides everything you need to do statistical work, but its structure can be difficult to master. This collection of concise, task-oriented recipes makes you productive with R immediately, with solutions ranging from basic tasks to input and output, general statistics, graphics, and linear regression.Each recipe addresses a specific problem, with a discussion that explains the solution and offers insight into how it works. If you’re a beginner, R Cookbook will help get you started. If you’re an experienced data programmer, it will jog your memory and expand your horizons. You’ll get the job done faster and learn more about R in the process.Create vectors, handle variables, and perform other basic functionsInput and output dataTackle data structures such as matrices, lists, factors, and data framesWork with probability, probability distributions, and random variablesCalculate statistics and confidence intervals, and perform statistical testsCreate a variety of graphic displaysBuild statistical models with linear regressions and analysis of variance (ANOVA)Explore advanced statistical techniques, such as finding clusters in your data"Wonderfully readable, R Cookbook serves not only as a solutions manual of sorts, but as a truly enjoyable way to explore the R language—one practical example at a time."—Jeffrey Ryan, software consultant and R package author},
	language = {English},
	publisher = {O'Reilly Media},
	author = {Teetor, Paul},
	month = mar,
	year = {2011}
}

@book{toomey_learning_2016,
	address = {Place of publication not identified},
	title = {Learning {Jupyter}},
	isbn = {978-1-78588-487-0},
	abstract = {Key FeaturesLearn to write, execute, and comment your live code and formulae all under one roof using this unique guideThis one-stop solution on Project Jupyter will teach you everything you need to know to perform scientific computation with easeThis easy-to-follow, highly practical guide lets you forget your worries in scientific application development by leveraging big data tools such as Apache Spark, Python, R etcBook DescriptionJupyter Notebook is a web-based environment that enables interactive computing in notebook documents. It allows you to create and share documents that contain live code, equations, visualizations, and explanatory text. The Jupyter Notebook system is extensively used in domains such as data cleaning and transformation, numerical simulation, statistical modeling, machine learning, and much more.This book starts with a detailed overview of the Jupyter Notebook system and its installation in different environments. Next we’ll help you will learn to integrate Jupyter system with different programming languages such as R, Python, JavaScript, and Julia and explore the various versions and packages that are compatible with the Notebook system. Moving ahead, you master interactive widgets, namespaces, and working with Jupyter in a multiuser mode.Towards the end, you will use Jupyter with a big data set and will apply all the functionalities learned throughout the book.What you will learnInstall and run the Jupyter Notebook system on your machineImplement programming languages such as R, Python, Julia, and JavaScript with Jupyter NotebookUse interactive widgets to manipulate and visualize data in real timeStart sharing your Notebook with colleaguesInvite your colleagues to work with you in the same NotebookOrganize your Notebook using Jupyter namespacesAccess big data in JupyterAbout the AuthorDan Toomey has been developing applications for over 20 years. He has worked in a variety of industries and size companies in roles from sole contributor to VP/CTO level. For the last 10 years or so, he has been contracting to companies in the eastern Massachusetts area. Dan has been contracting under Dan Toomey Software Corp. Again, as a contractor developer in the area. Dan has also written R for Data Sciences with Packt Publishing.Table of ContentsIntroduction to JupyterJupyter Python ScriptingJupyter R ScriptingJupyter Julia ScriptingJupyter JavaScript CodingInteractive WidgetsSharing and Converting Jupyter NotebooksMultiuser Jupyter NotebooksJupyter ScalaJupyter and Big Data},
	language = {English},
	publisher = {Packt Publishing - ebooks Account},
	author = {Toomey, Dan},
	month = nov,
	year = {2016}
}

@book{summerfield_programming_2009,
	address = {Upper Saddle River, NJ},
	edition = {Edição: 2},
	title = {Programming in {Python} 3: {A} {Complete} {Introduction} to the {Python} {Language}},
	isbn = {978-0-321-68056-3},
	shorttitle = {Programming in {Python} 3},
	abstract = {A Fully Revised Edition Featuring New Material on Coroutines, Debugging, Testing, Parsing, String Formatting, and More  � Python 3 is the best version of the language yet: It is more powerful, convenient, consistent, and expressive than ever before. Now, leading Python programmer Mark Summerfield demonstrates how to write code that takes full advantage of Python 3's features and idioms. Programming in Python 3, Second Edition, brings together all the knowledge you need to write any program, use any standard or third-party Python 3 library, and create new library modules of your own. � Summerfield draws on his many years of Python experience to share deep insights into Python 3 development you won't find anywhere else. He begins by illuminating Python's "beautiful heart": the eight key elements of Python you need to write robust, high-performance programs. Building on these core elements, he introduces new topics designed to strengthen your practical expertise-one concept and hands-on example at a time. Coverage includes  Developing in Python using procedural, objectoriented, and functional programming paradigms Creating custom packages and modules Writing and reading binary, text, and XML files, including optional compression, random access, and text and XML parsing Leveraging advanced data types, collections, control structures, and functions Spreading program workloads across multiple processes and threads Programming SQL databases and key--value DBM files Debugging techniques-and using Test Driven Development to avoid bugs in the first place Utilizing Python's regular expression mini-language and module Parsing techniques, including how to use the third-party PyParsing and PLY modules Building usable, efficient, GUI-based applications Advanced programming techniques, including generators, function and class decorators, context managers, descriptors, abstract base classes, metaclasses, coroutines, and more � Programming in Python 3, Second Edition, serves as both tutorial and language reference. It assumes some prior programming experience, and is accompanied by extensive downloadable example code-all of it tested with Python 3 on Windows, Linux, and Mac OS X. This edition covers Python 3.0 and 3.1, and due to the Python language moratorium it is also valid for Python 3.2 which has the same language as Python 3.1.},
	language = {Inglês},
	publisher = {Addison-Wesley Professional},
	author = {Summerfield, Mark},
	month = nov,
	year = {2009}
}

@book{aamodt_general-purpose_2018,
	title = {General-{Purpose} {Graphics} {Processor} {Architectures}},
	isbn = {978-1-62705-923-7},
	abstract = {Originally developed to support video games, graphics processor units (GPUs) are now increasingly used for general-purpose (non-graphics) applications ranging from machine learning to mining of cryptographic currencies. GPUs can achieve improved performance and efficiency versus central processing units (CPUs) by dedicating a larger fraction of hardware resources to computation. In addition, their general-purpose programmability makes contemporary GPUs appealing to software developers in comparison to domain-specific accelerators. This book provides an introduction to those interested in studying the architecture of GPUs that support general-purpose computing. It collects together information currently only found among a wide range of disparate sources. The authors led development of the GPGPU-Sim simulator widely used in academic research on GPU architectures.The first chapter of this book describes the basic hardware structure of GPUs and provides a brief overview of their history. Chapter 2 provides a summary of GPU programming models relevant to the rest of the book. Chapter 3 explores the architecture of GPU compute cores. Chapter 4 explores the architecture of the GPU memory system. After describing the architecture of existing systems, Chapters {\textbackslash}ref\{ch03\} and {\textbackslash}ref\{ch04\} provide an overview of related research. Chapter 5 summarizes cross-cutting research impacting both the compute core and memory system.This book should provide a valuable resource for those wishing to understand the architecture of graphics processor units (GPUs) used for acceleration of general-purpose applications and to those who want to obtain an introduction to the rapidly growing body of research exploring how to improve the architecture of these GPUs.},
	language = {English},
	publisher = {Morgan \& Claypool},
	author = {Aamodt, Tor M. and Fung, Wilson Wai Lun and Rogers, Timothy G.},
	editor = {Martonosi, Margaret},
	year = {2018}
}

@book{jeffers_intel_2016,
	address = {Cambridge, MA},
	edition = {Edição: 2},
	title = {Intel {Xeon} {Phi} {Processor} {High} {Performance} {Programming}: {Knights} {Landing} {Edition}},
	isbn = {978-0-12-809194-4},
	shorttitle = {Intel {Xeon} {Phi} {Processor} {High} {Performance} {Programming}},
	abstract = {Intel Xeon Phi Processor High Performance Programming is an all-in-one source of information for programming the Second-Generation Intel Xeon Phi product family also called Knights Landing. The authors provide detailed and timely Knights Landingspecific details, programming advice, and real-world examples. The authors distill their years of Xeon Phi programming experience coupled with insights from many expert customers - Intel Field Engineers, Application Engineers, and Technical Consulting Engineers - to create this authoritative book on the essentials of programming for Intel Xeon Phi products. Intel® Xeon Phi™ Processor High-Performance Programming is useful even before you ever program a system with an Intel Xeon Phi processor. To help ensure that your applications run at maximum efficiency, the authors emphasize key techniques for programming any modern parallel computing system whether based on Intel Xeon processors, Intel Xeon Phi processors, or other high-performance microprocessors. Applying these techniques will generally increase your program performance on any system and prepare you better for Intel Xeon Phi processors.A practical guide to the essentials for programming Intel Xeon Phi processorsDefinitive coverage of the Knights Landing architecturePresents best practices for portable, high-performance computing and a familiar and proven threads and vectors programming modelIncludes real world code examples that highlight usages of the unique aspects of this new highly parallel and high-performance computational productCovers use of MCDRAM, AVX-512, Intel® Omni-Path fabric, many-cores (up to 72), and many threads (4 per core)Covers software developer tools, libraries and programming modelsCovers using Knights Landing as a processor and a coprocessor},
	language = {Inglês},
	publisher = {Morgan Kaufmann},
	author = {Jeffers, James and Reinders, James and Sodani, Avinash},
	month = jul,
	year = {2016}
}

@book{kirk_programming_2012,
	address = {Amsterdam},
	edition = {Edição: 2},
	title = {Programming {Massively} {Parallel} {Processors}: {A} {Hands}-on {Approach}},
	isbn = {978-0-12-415992-1},
	shorttitle = {Programming {Massively} {Parallel} {Processors}},
	abstract = {Programming Massively Parallel Processors: A Hands-on Approach, Second Edition, teaches students how to program massively parallel processors. It offers a detailed discussion of various techniques for constructing parallel programs. Case studies are used to demonstrate the development process, which begins with computational thinking and ends with effective and efficient parallel programs. This guide shows both student and professional alike the basic concepts of parallel programming and GPU architecture. Topics of performance, floating-point format, parallel patterns, and dynamic parallelism are covered in depth. This revised edition contains more parallel programming examples, commonly-used libraries such as Thrust, and explanations of the latest tools. It also provides new coverage of CUDA 5.0, improved performance, enhanced development tools, increased hardware support, and more; increased coverage of related technology, OpenCL and new material on algorithm patterns, GPU clusters, host programming, and data parallelism; and two new case studies (on MRI reconstruction and molecular visualization) that explore the latest applications of CUDA and GPUs for scientific research and high-performance computing. This book should be a valuable resource for advanced students, software engineers, programmers, and hardware engineers.New coverage of CUDA 5.0, improved performance, enhanced development tools, increased hardware support, and moreIncreased coverage of related technology, OpenCL and new material on algorithm patterns, GPU clusters, host programming, and data parallelismTwo new case studies (on MRI reconstruction and molecular visualization) explore the latest applications of CUDA and GPUs for scientific research and high-performance computing},
	language = {English},
	publisher = {Morgan Kaufmann},
	author = {Kirk, David B. and Hwu, Wen-mei W.},
	year = {2012}
}

@book{chandrasekaran_openacc_2017,
	address = {Boston},
	edition = {Edição: 1},
	title = {{OpenACC} for {Programmers}: {Concepts} and {Strategies}},
	isbn = {978-0-13-469428-3},
	shorttitle = {{OpenACC} for {Programmers}},
	abstract = {The Complete Guide to OpenACC for Massively Parallel Programming    Scientists and technical professionals can use OpenACC to leverage the immense power of modern GPUs without the complexity traditionally associated with programming them.  OpenACC™ for Programmers  is one of the first comprehensive and practical overviews of OpenACC for massively parallel programming.   This book integrates contributions from 19 leading parallel-programming experts from academia, public research organizations, and industry. The authors and editors explain each key concept behind OpenACC, demonstrate how to use essential OpenACC development tools, and thoroughly explore each OpenACC feature set.   Throughout, you’ll find realistic examples, hands-on exercises, and case studies showcasing the efficient use of OpenACC language constructs. You’ll discover how OpenACC’s language constructs can be translated to maximize application performance, and how its standard interface can target multiple platforms via widely used programming languages.   Each chapter builds on what you’ve already learned, helping you build practical mastery one step at a time, whether you’re a GPU programmer, scientist, engineer, or student. All example code and exercise solutions are available for download at GitHub.  Discover how OpenACC makes scalable parallel programming easier and more practical  Walk through the OpenACC spec and learn how OpenACC directive syntax is structured Get productive with OpenACC code editors, compilers, debuggers, and performance analysis tools Build your first real-world OpenACC programs Exploit loop-level parallelism in OpenACC, understand the levels of parallelism available, and maximize accuracy or performance Learn how OpenACC programs are compiled Master OpenACC programming best practices Overcome common performance, portability, and interoperability challenges Efficiently distribute tasks across multiple processors   Register your product at informit.com/register for convenient access to downloads, updates, and/or corrections as they become available.},
	language = {English},
	publisher = {Addison-Wesley Professional},
	author = {Chandrasekaran, Sunita and Juckeland, Guido},
	year = {2017}
}

@book{wilke_fundamentals_2019,
	address = {S.l.},
	edition = {Edição: 1},
	title = {Fundamentals of {Data} {Visualization}: {A} {Primer} on {Making} {Informative} and {Compelling} {Figures}},
	isbn = {978-1-4920-3108-6},
	shorttitle = {Fundamentals of {Data} {Visualization}},
	abstract = {Effective visualization is the best way to communicate information from the increasingly large and complex datasets in the natural and social sciences. But with the increasing power of visualization software today, scientists, engineers, and business analysts often have to navigate a bewildering array of visualization choices and options.This practical book takes you through many commonly encountered visualization problems, and it provides guidelines on how to turn large datasets into clear and compelling figures. What visualization type is best for the story you want to tell? How do you make informative figures that are visually pleasing? Author Claus O. Wilke teaches you the elements most critical to successful data visualization.Explore the basic concepts of color as a tool to highlight, distinguish, or represent a valueUnderstand the importance of redundant coding to ensure you provide key information in multiple waysUse the book’s visualizations directory, a graphical guide to commonly used types of data visualizationsGet extensive examples of good and bad figuresLearn how to use figures in a document or report and how employ them effectively to tell a compelling story},
	language = {English},
	publisher = {O'Reilly Media},
	author = {Wilke, Claus O.},
	year = {2019}
}

@book{kazil_data_2016,
	edition = {Edição: 1},
	title = {Data {Wrangling} with {Python}: {Tips} and {Tools} to {Make} {Your} {Life} {Easier}},
	shorttitle = {Data {Wrangling} with {Python}},
	abstract = {How do you take your data analysis skills beyond Excel to the next level? By learning just enough Python to get stuff done. This hands-on guide shows non-programmers like you how to process information that’s initially too messy or difficult to access. You don't need to know a thing about the Python programming language to get started.Through various step-by-step exercises, you’ll learn how to acquire, clean, analyze, and present data efficiently. You’ll also discover how to automate your data process, schedule file- editing and clean-up tasks, process larger datasets, and create compelling stories with data you obtain.Quickly learn basic Python syntax, data types, and language conceptsWork with both machine-readable and human-consumable dataScrape websites and APIs to find a bounty of useful informationClean and format data to eliminate duplicates and errors in your datasetsLearn when to standardize data and when to test and script data cleanupExplore and analyze your datasets with new Python libraries and techniquesUse Python solutions to automate your entire data-wrangling process},
	language = {English},
	publisher = {O'Reilly Media},
	author = {Kazil, Jacqueline and Jarmul, Katharine},
	year = {2016}
}

@book{aggarwal_neural_2018,
	address = {Cham},
	edition = {Edição: 1st ed. 2018},
	title = {Neural {Networks} and {Deep} {Learning}: {A} {Textbook}},
	isbn = {978-3-319-94462-3},
	shorttitle = {Neural {Networks} and {Deep} {Learning}},
	abstract = {This book covers both classical and modern models in deep learning. The primary focus is on the theory and algorithms of deep learning. The theory and algorithms of neural networks are particularly important for understanding important concepts, so that one can understand the important design concepts of neural architectures in different applications. Why do neural networks work? When do they work better than off-the-shelf machine-learning models? When is depth useful? Why is training neural networks so hard? What are the pitfalls? The book  is also rich in discussing different applications in order to give the practitioner a flavor of how neural architectures are designed for different types of problems. Applications associated with many different areas like recommender systems, machine translation, image captioning, image classification, reinforcement-learning based gaming, and text analytics are covered. The chapters of this book span three categories: The basics of neural networks:  Many traditional machine learning models can be understood as special cases of neural networks.  An emphasis is placed in the first two chapters on understanding the relationship between traditional machine learning and neural networks. Support vector machines, linear/logistic regression, singular value decomposition, matrix factorization, and recommender systems are shown to be special cases of neural networks. These methods are studied together with recent feature engineering methods like word2vec. Fundamentals of neural networks: A detailed discussion of training and regularization is provided in Chapters 3 and 4. Chapters 5 and 6 present radial-basis function (RBF) networks and restricted Boltzmann machines. Advanced topics in neural networks: Chapters 7 and 8 discuss recurrent neural networks and convolutional neural networks. Several advanced topics like deep reinforcement learning, neural Turing machines, Kohonen self-organizing maps, and generative adversarial networks are introduced in Chapters 9 and 10. The book is written for graduate students, researchers, and practitioners.   Numerous exercises are available along with a solution manual to aid in classroom teaching. Where possible, an application-centric view is highlighted in order to provide an understanding of the practical uses of each class of techniques.},
	language = {English},
	publisher = {Springer},
	author = {Aggarwal, Charu C.},
	year = {2018}
}

@book{moore_statistics_2015,
	address = {New York, NY},
	edition = {First edition},
	title = {Statistics in practice},
	isbn = {978-1-4641-5181-1 978-1-4641-5238-2 978-1-4641-7451-3},
	abstract = {Statistics in Practice is an exciting new addition to W.H. Freeman's introductory statistics list. Co-authored by David Moore, it maintains his pioneering data analysis approach but incorporates significant changes designed to help students. Statistics in Practice introduces data collection early, covers tests of proportions first before tests of means, and engages students with its conversational writing style. SIP is a modern approach to the introductory statistics course, clearly showing the importance of statistics to students during their academic life and beyond.--},
	publisher = {W. H. Freeman and Company, a Macmillan Higher Education Company},
	author = {Moore, David S. and Notz, William I. and Fligner, Michael A.},
	year = {2015},
	note = {OCLC: ocn920570858},
	keywords = {Graphic methods, Mathematical statistics, Statistics, Textbooks}
}
